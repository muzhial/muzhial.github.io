<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="应无所住而生其心">
    <meta name="author" content="muzhi :) 木之">
    
    <title>
        
            vLLM 源码阅读 (part1) |
        
        muzhi.al
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
        <link rel="shortcut icon" href="/images/logo.png">
    
    
<link rel="stylesheet" href="/font/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/font/css/regular.min.css">

    
<link rel="stylesheet" href="/font/css/solid.min.css">

    
<link rel="stylesheet" href="/font/css/brands.min.css">

    
    <script class="keep-theme-configurations">
    const KEEP = window.KEEP || {}
    KEEP.hexo_config = {"hostname":"example.com","root":"/","language":"en"}
    KEEP.theme_config = {"toc":{"enable":false,"number":false,"expand_all":false,"init_open":true,"layout":"right"},"style":{"primary_color":"#0066cc","logo":"/images/logo.png","favicon":"/images/logo.png","avatar":"/images/favicon-192x192.png","first_screen":{"enable":false,"header_transparent":false,"background_img":"/images/bg.svg","description":"应无所住而生其心","hitokoto":false},"scroll":{"progress_bar":false,"percent":false}},"local_search":{"enable":false,"preload":false},"code_block":{"tools":{"enable":false,"style":"default"},"highlight_theme":"default"},"pjax":{"enable":false},"lazyload":{"enable":false},"comment":{"enable":false,"use":"valine","valine":{"appid":null,"appkey":null,"server_urls":null,"placeholder":null},"gitalk":{"github_id":null,"github_admins":null,"repository":null,"client_id":null,"client_secret":null,"proxy":null},"twikoo":{"env_id":null,"region":null,"version":"1.6.21"},"waline":{"server_url":null,"reaction":false,"version":2},"giscus":{"repo":null,"repo_id":null,"category":"Announcements","category_id":null,"reactions_enabled":false}},"post":{"author_label":{"enable":false,"auto":false,"custom_label_list":["Trainee","Engineer","Architect"]},"word_count":{"wordcount":true,"min2read":false},"datetime_format":"YYYY-MM-DD","copyright_info":false,"share":false,"reward":{"enable":false,"img_link":null,"text":null}},"website_count":{"busuanzi_count":{"enable":false,"site_uv":false,"site_pv":false,"page_pv":false}},"version":"3.8.2"}
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"}
    KEEP.language_code_block = {"copy":"Copy code","copied":"Copied","fold":"Fold code block","folded":"Folded"}
    KEEP.language_copy_copyright = {"copy":"Copy copyright info","copied":"Copied","title":"Original article title","author":"Original article author","link":"Original article link"}
  </script>
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="木之" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    

    
</div>


<main class="page-container border-box">

    <!-- home first screen  -->
    

    <!-- page content -->
    <div class="page-main-content border-box">
        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="border-box header-content">
        <div class="left border-box">
            
                <a class="logo-image border-box" href="/">
                    <img src="/images/logo.png">
                </a>
            
            <a class="site-name border-box" href="/">
               muzhi.al
            </a>
        </div>

        <div class="right border-box">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                主页
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                归档
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/slice"
                            >
                                断章
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/about"
                            >
                                关于
                            </a>
                        </li>
                    
                    
                </ul>
            </div>
            <div class="mobile">
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">主页</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">归档</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/slice">断章</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/about">关于</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle border-box">

            <div class="main-content border-box">

                

                    <div class="fade-in-down-animation">
    <div class="post-page-container border-box">

        <div class="article-content-container border-box">

            

            <div class="article-content-bottom border-box">
                
                    <div class="article-title">
                        vLLM 源码阅读 (part1)
                    </div>
                

                
                    <div class="article-header border-box">
                        <!-- 
                            <div class="avatar-box border-box">
                                <img src="/images/favicon-192x192.png">
                            </div>
                         -->
                        <div class="info-box">
                            <!-- <div class="author">
                                <span class="name">muzhi :) 木之</span>
                                
                            </div> -->
                            <div class="meta-info border-box">
                                

<div class="article-meta-info-container border-box post">
    <div class="article-meta-info border-box">
        


        
            <span class="meta-info-item article-create-date">
                <i class="icon fa-solid fa-calendar-check"></i>&nbsp;
                <span class="pc">2025-05-03</span>
                <span class="mobile">2025-05-03 11:04</span>
            </span>

            <!-- <span class="meta-info-item article-update-date">
                <i class="icon fa-solid fa-file-pen"></i>&nbsp;
                <span class="pc" data-updated="Thu May 08 2025 00:10:45 GMT+0800">2025-05-08</span>
            </span> -->
        

        
            <span class="meta-info-item article-category border-box"><i class="icon fas fa-folder"></i>&nbsp;
                <ul class="article-category-ul">
                    
                            <li class="category-item"><a href="/categories/infra/">infra</a></li>
                        
                    
                </ul>
            </span>
        

        
            <span class="article-tag meta-info-item border-box">
                <i class="icon fas fa-tags"></i>&nbsp;
                <ul class="article-tag-ul">
                    
                            <li class="tag-item"><span class="tag-separator"><i class="icon fas fa-hashtag"></i></span><a href="/tags/vLLM/">vLLM</a></li>
                        
                    
                </ul>
            </span>
        

        
        
            <span class="meta-info-item article-wordcount">
                <i class="icon fas fa-file-word"></i>&nbsp;<span>3.8k Words</span>
            </span>
        
        
        
    </div>

    
</div>

                            </div>
                        </div>
                    </div>
                

                <div class="article-content keep-markdown-body">
                    

                    <blockquote>
<p><strong>说明：</strong>基于 vLLM v0.7.3，commit id: <code>ed6e9075d31e32c8548b480a47d1ffb77da1f54c (HEAD, tag: v0.7.3)</code></p>
</blockquote>
<h2 id="PagedAttention"><a href="#PagedAttention" class="headerlink" title="PagedAttention"></a>PagedAttention</h2><ul>
<li><p>提出：解决 KV Cache 不连续导致的利用率不高问题。</p>
</li>
<li><p>KV Cache 利用率不高的问题：（可参考 pagedattention paper）</p>
<ul>
<li>事先不知道请求的长度（prompt + output），如果提前分配过大的空间会导致浪费，产生内部碎片（internal fragmentation）；过小又无法分配给其他请求，产生外部碎片（external fragmentation）。</li>
<li>无法共享空间，如 beam search 等解码算法会针对一个请求生成多个输出，现有系统无法使多个输出共享一个 prompt。</li>
</ul>
</li>
<li><p>原理</p>
<ul>
<li><a class="link" target="_blank" rel="noopener" href="https://blog.vllm.ai/2023/06/20/vllm.html">https://blog.vllm.ai/2023/06/20/vllm.html<i class="fas fa-external-link-alt"></i></a><img src="pagedattention_annimation1.gif" width="75%"></li>
</ul>
</li>
</ul>
<h2 id="vLLM-的推理"><a href="#vLLM-的推理" class="headerlink" title="vLLM 的推理"></a>vLLM 的推理</h2><p>推理的示例程序：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> LLM, SamplingParams</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">prompts = [</span><br><span class="line">    (<span class="string">"A robot may not injure a human being"</span>,</span><br><span class="line">     SamplingParams(temperature=<span class="number">0.0</span>, logprobs=<span class="number">1</span>, prompt_logprobs=<span class="number">1</span>)),</span><br><span class="line">    (<span class="string">"To be or not to be,"</span>,</span><br><span class="line">     SamplingParams(temperature=<span class="number">0.8</span>, top_k=<span class="number">5</span>, presence_penalty=<span class="number">0.2</span>)),</span><br><span class="line">    (<span class="string">"What is the meaning of life?"</span>,</span><br><span class="line">     SamplingParams(temperature=<span class="number">0.8</span>, top_k=<span class="number">5</span>, presence_penalty=<span class="number">0.2</span>)),</span><br><span class="line">    <span class="comment"># SamplingParams(n=2,</span></span><br><span class="line">    <span class="comment">#             best_of=5,</span></span><br><span class="line">    <span class="comment">#             temperature=0.8,</span></span><br><span class="line">    <span class="comment">#             top_p=0.95,</span></span><br><span class="line">    <span class="comment">#             frequency_penalty=0.1)),</span></span><br><span class="line">]</span><br><span class="line">prompts, sampling_paras = <span class="built_in">zip</span>(*prompts)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以 Qwen2.5 为例</span></span><br><span class="line">llm = LLM(model=<span class="string">"Qwen/Qwen2.5-3B-Instruct"</span>)</span><br><span class="line"></span><br><span class="line">outputs = llm.generate(prompts, sampling_paras)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> output <span class="keyword">in</span> outputs:</span><br><span class="line">    prompt = output.prompt</span><br><span class="line">    generated_text = output.outputs[<span class="number">0</span>].text</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Prompt: <span class="subst">{prompt!r}</span>, Generated text: <span class="subst">{generated_text!r}</span>"</span>)</span><br></pre></td></tr></table></figure>

<h2 id="初始化流程"><a href="#初始化流程" class="headerlink" title="初始化流程"></a>初始化流程</h2><h3 id="接口类-LLM"><a href="#接口类-LLM" class="headerlink" title="接口类 LLM"></a>接口类 LLM</h3><p>LLM 类的初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># entrypoints/llm.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LLM</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">...</span>):</span><br><span class="line">        engine_args = EngineArgs(</span><br><span class="line">            ...</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># Logic to switch between engines is done at runtime instead of import</span></span><br><span class="line">        <span class="comment"># to avoid import order issues</span></span><br><span class="line">        <span class="comment"># 获取 engine 的类，如果指定环境变量 `VLLM_USE_V1` 则使用 V1LLMEngine，默认使用 LLMEngine</span></span><br><span class="line">        self.engine_class = self.get_engine_class()</span><br><span class="line">        <span class="comment"># 调用 LLMEngine::from_engine_args，进行 engine 的初始化</span></span><br><span class="line">        self.llm_engine = self.engine_class.from_engine_args(</span><br><span class="line">            engine_args, usage_context=UsageContext.LLM_CLASS)</span><br><span class="line"></span><br><span class="line">        self.request_counter = Counter()</span><br></pre></td></tr></table></figure>

<p>构造 LLMEngine：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># engine/llm_engine.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LLMEngine</span>:</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_engine_args</span>(<span class="params"></span></span><br><span class="line"><span class="params">        cls,</span></span><br><span class="line"><span class="params">        engine_args: EngineArgs,</span></span><br><span class="line"><span class="params">        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,</span></span><br><span class="line"><span class="params">        stat_loggers: <span class="type">Optional</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, StatLoggerBase]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="string">"LLMEngine"</span>:</span><br><span class="line">        <span class="string">"""Creates an LLM engine from the engine arguments."""</span></span><br><span class="line">        <span class="comment"># EngineArgs 类的 create_engine_config 函数，创建初始化各个 config</span></span><br><span class="line">        <span class="comment"># 如：ModelConfig, CacheConfig, ParallelConfig, SchedulerConfig, LoRAConfig 等，最后将上述 config 组合进 VllmConfig</span></span><br><span class="line">        <span class="comment"># VllmConfig 初始化会调用 `current_platform.check_and_update_config(self)` 设置 worker_cls='vllm.worker.worker.Worker'</span></span><br><span class="line">        engine_config = engine_args.create_engine_config(usage_context)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># executor 的 backend 由 ParallelConfig 的 __post_init__ 设置</span></span><br><span class="line">        <span class="comment"># world_size=1，默认则 backend="uni"，对应的 executor 为 UniProcExecutor (in executor/uniproc_executor.py)</span></span><br><span class="line">        executor_class = cls._get_executor_cls(engine_config)</span><br><span class="line">        <span class="comment"># Create the LLM engine.</span></span><br><span class="line">        engine = cls(</span><br><span class="line">            vllm_config=engine_config,</span><br><span class="line">            executor_class=executor_class,</span><br><span class="line">            log_stats=<span class="keyword">not</span> engine_args.disable_log_stats,</span><br><span class="line">            usage_context=usage_context,</span><br><span class="line">            stat_loggers=stat_loggers,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> engine</span><br></pre></td></tr></table></figure>

<h3 id="LLMEngine-类"><a href="#LLMEngine-类" class="headerlink" title="LLMEngine 类"></a>LLMEngine 类</h3><p><code>LLMEngine</code> 初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># engine/llm_engine.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LLMEngine</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        vllm_config: VllmConfig,</span></span><br><span class="line"><span class="params">        executor_class: <span class="type">Type</span>[ExecutorBase],</span></span><br><span class="line"><span class="params">        log_stats: <span class="built_in">bool</span>,</span></span><br><span class="line"><span class="params">        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,</span></span><br><span class="line"><span class="params">        stat_loggers: <span class="type">Optional</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, StatLoggerBase]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        input_registry: InputRegistry = INPUT_REGISTRY,</span></span><br><span class="line"><span class="params">        mm_registry: MultiModalRegistry = MULTIMODAL_REGISTRY,</span></span><br><span class="line"><span class="params">        use_cached_outputs: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.vllm_config = vllm_config</span><br><span class="line">        self.model_config = vllm_config.model_config</span><br><span class="line">        self.cache_config = vllm_config.cache_config</span><br><span class="line">        self.lora_config = vllm_config.lora_config</span><br><span class="line">        self.parallel_config = vllm_config.parallel_config</span><br><span class="line">        self.scheduler_config = vllm_config.scheduler_config</span><br><span class="line">        self.device_config = vllm_config.device_config</span><br><span class="line">        self.speculative_config = vllm_config.speculative_config  <span class="comment"># noqa</span></span><br><span class="line">        self.load_config = vllm_config.load_config</span><br><span class="line">        self.decoding_config = vllm_config.decoding_config <span class="keyword">or</span> DecodingConfig(  <span class="comment"># noqa</span></span><br><span class="line">        )</span><br><span class="line">        self.prompt_adapter_config = vllm_config.prompt_adapter_config  <span class="comment"># noqa</span></span><br><span class="line">        self.observability_config = vllm_config.observability_config <span class="keyword">or</span> ObservabilityConfig(  <span class="comment"># noqa</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.log_stats = log_stats</span><br><span class="line">        self.use_cached_outputs = use_cached_outputs</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.model_config.skip_tokenizer_init:</span><br><span class="line">            self.tokenizer = self._init_tokenizer()</span><br><span class="line">            self.detokenizer = Detokenizer(self.tokenizer)</span><br><span class="line">            tokenizer_group = self.get_tokenizer_group()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.tokenizer = <span class="literal">None</span></span><br><span class="line">            self.detokenizer = <span class="literal">None</span></span><br><span class="line">            tokenizer_group = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Ensure that the function doesn't contain a reference to self,</span></span><br><span class="line">        <span class="comment"># to avoid engine GC issues</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">get_tokenizer_for_seq</span>(<span class="params">sequence: <span class="type">Sequence</span></span>) -&gt; AnyTokenizer:</span><br><span class="line">            <span class="keyword">assert</span> tokenizer_group, (<span class="string">"tokenizer_group cannot be None, "</span></span><br><span class="line">                                     <span class="string">"make sure skip_tokenizer_init is False"</span>)</span><br><span class="line">            <span class="keyword">return</span> tokenizer_group.get_lora_tokenizer(sequence.lora_request)</span><br><span class="line"></span><br><span class="line">        self.seq_counter = Counter()</span><br><span class="line">        self.generation_config_fields = (</span><br><span class="line">            self.model_config.try_get_generation_config())</span><br><span class="line"></span><br><span class="line">        self.input_preprocessor = InputPreprocessor(self.model_config,</span><br><span class="line">                                                    self.tokenizer,</span><br><span class="line">                                                    mm_registry)</span><br><span class="line"></span><br><span class="line">        self.input_registry = input_registry</span><br><span class="line">        self.input_processor = input_registry.create_input_processor(</span><br><span class="line">            self.model_config)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化 executor</span></span><br><span class="line">        <span class="comment"># 如：UniProcExecutor (in executor/uniproc_executor.py)</span></span><br><span class="line">        self.model_executor = executor_class(vllm_config=vllm_config, )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.model_config.runner_type != <span class="string">"pooling"</span>:</span><br><span class="line">            <span class="comment"># 初始化 kv cache</span></span><br><span class="line">            <span class="comment"># 调用 Worker::determine_num_available_blocks</span></span><br><span class="line">            <span class="comment"># 1. 模型先 forward 一次，得到剩余的可分配的 KV Cache 显存大小。</span></span><br><span class="line">            <span class="comment"># 2. 计算 cache_block_size。</span></span><br><span class="line">            <span class="comment"># 3. 可用的 KV Cache 显存大小 / cache_block_size = num_gpu_blocks</span></span><br><span class="line">            <span class="comment">#    可用的 CPU swap_space_bytes / cache_block_size = bum_cpu_blocks</span></span><br><span class="line">            <span class="comment"># 4. 根据 num_gpu_blocks, num_cpu_blocks 调用 Worker 的 _init_cache_engine 方法。</span></span><br><span class="line">            <span class="comment"># 会初始化 CacheEngine, CacheEngine 用作 KV Cache 的管理。</span></span><br><span class="line">            <span class="comment"># gpu_cache: List[torch.Tensor] 是 List[(num_blocks, block_size, num_kv_heads, head_size)]</span></span><br><span class="line">            self._initialize_kv_caches()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.tokenizer:</span><br><span class="line">            <span class="comment"># Ping the tokenizer to ensure liveness if it runs in a</span></span><br><span class="line">            <span class="comment"># different process.</span></span><br><span class="line">            self.tokenizer.ping()</span><br><span class="line"></span><br><span class="line">        self.cached_scheduler_outputs = [</span><br><span class="line">            SchedulerOutputState()</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.parallel_config.pipeline_parallel_size)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        self.scheduler_contexts = [</span><br><span class="line">            SchedulerContext(multi_step_stream_outputs=self.scheduler_config.</span><br><span class="line">                             multi_step_stream_outputs)</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.parallel_config.pipeline_parallel_size)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.model_config.use_async_output_proc:</span><br><span class="line">            process_model_outputs = weak_bind(self._process_model_outputs)</span><br><span class="line"></span><br><span class="line">            self.async_callbacks = [</span><br><span class="line">                partial(process_model_outputs,</span><br><span class="line">                        ctx=self.scheduler_contexts[v_id])</span><br><span class="line">                <span class="keyword">for</span> v_id <span class="keyword">in</span> <span class="built_in">range</span>(self.parallel_config.pipeline_parallel_size)</span><br><span class="line">            ]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.async_callbacks = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Currently used by AsyncLLMEngine to ensure quick append</span></span><br><span class="line">        <span class="comment"># of request outputs to asyncio queues</span></span><br><span class="line">        self.process_request_outputs_callback: <span class="type">Optional</span>[<span class="type">Callable</span>] = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Create the scheduler.</span></span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> the cache_config here have been updated with the numbers of</span></span><br><span class="line">        <span class="comment"># GPU and CPU blocks, which are profiled in the distributed executor.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(self.vllm_config.scheduler_config.scheduler_cls, <span class="built_in">str</span>):</span><br><span class="line">            Scheduler = resolve_obj_by_qualname(</span><br><span class="line">                self.vllm_config.scheduler_config.scheduler_cls)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Scheduler = self.vllm_config.scheduler_config.scheduler_cls</span><br><span class="line">        <span class="comment"># vllm/core/scheduler.py</span></span><br><span class="line">        <span class="comment"># 初始化 Scheduler</span></span><br><span class="line">        self.scheduler = [</span><br><span class="line">            Scheduler(</span><br><span class="line">                self.scheduler_config, self.cache_config, self.lora_config,</span><br><span class="line">                self.parallel_config.pipeline_parallel_size,</span><br><span class="line">                self.async_callbacks[v_id]</span><br><span class="line">                <span class="keyword">if</span> self.model_config.use_async_output_proc <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line">            <span class="keyword">for</span> v_id <span class="keyword">in</span> <span class="built_in">range</span>(self.parallel_config.pipeline_parallel_size)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        self.tracer = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> self.observability_config.otlp_traces_endpoint:</span><br><span class="line">            self.tracer = init_tracer(</span><br><span class="line">                <span class="string">"vllm.llm_engine"</span>,</span><br><span class="line">                self.observability_config.otlp_traces_endpoint)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Create sequence output processor, e.g. for beam search or</span></span><br><span class="line">        <span class="comment"># speculative decoding.</span></span><br><span class="line">        self.output_processor = (</span><br><span class="line">            SequenceGroupOutputProcessor.create_output_processor(</span><br><span class="line">                self.scheduler_config,</span><br><span class="line">                self.detokenizer,</span><br><span class="line">                self.scheduler,</span><br><span class="line">                self.seq_counter,</span><br><span class="line">                get_tokenizer_for_seq,</span><br><span class="line">                stop_checker=StopChecker(</span><br><span class="line">                    self.scheduler_config.max_model_len,</span><br><span class="line">                    get_tokenizer_for_seq,</span><br><span class="line">                ),</span><br><span class="line">            ))</span><br><span class="line"></span><br><span class="line">        self.seq_id_to_seq_group: <span class="type">Dict</span>[<span class="built_in">str</span>, SequenceGroupBase] = {}</span><br></pre></td></tr></table></figure>

<p><code>LLMEngine</code> 初始化流程涉及如下几个核心类或组件的初始化。</p>
<ol>
<li>executor, Worker 初始化</li>
</ol>
<p>executor 初始化时序图如下：</p>
<img src="executor_worker_init.png">

<p><code>Worker</code> 初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vllm/worker/worker.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Worker</span>(<span class="title class_ inherited__">LocalOrDistributedWorkerBase</span>):</span><br><span class="line">    <span class="string">"""A worker class that executes (a partition of) the model on a GPU.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Each worker is associated with a single GPU. The worker is responsible for</span></span><br><span class="line"><span class="string">    maintaining the KV cache and executing the model on the GPU. In case of</span></span><br><span class="line"><span class="string">    distributed inference, each worker is assigned a partition of the model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        vllm_config: VllmConfig,</span></span><br><span class="line"><span class="params">        local_rank: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        rank: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        distributed_init_method: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">        is_driver_worker: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        model_runner_cls: <span class="type">Optional</span>[<span class="type">Type</span>[GPUModelRunnerBase]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        WorkerBase.__init__(self, vllm_config)</span><br><span class="line">        self.parallel_config.rank = rank</span><br><span class="line">        self.local_rank = local_rank</span><br><span class="line">        self.rank = rank</span><br><span class="line">        self.distributed_init_method = distributed_init_method</span><br><span class="line">        self.is_driver_worker = is_driver_worker</span><br><span class="line"></span><br><span class="line">        ModelRunnerClass: <span class="type">Type</span>[GPUModelRunnerBase] = ModelRunner</span><br><span class="line">        <span class="keyword">if</span> model_config.runner_type == <span class="string">"pooling"</span>:</span><br><span class="line">            ModelRunnerClass = PoolingModelRunner</span><br><span class="line">        <span class="keyword">elif</span> self.model_config.is_encoder_decoder:</span><br><span class="line">            ModelRunnerClass = EncoderDecoderModelRunner</span><br><span class="line">        <span class="comment"># 初始化 model runner</span></span><br><span class="line">        self.model_runner: GPUModelRunnerBase = ModelRunnerClass(</span><br><span class="line">            vllm_config=self.vllm_config,</span><br><span class="line">            kv_cache_dtype=self.cache_config.cache_dtype,</span><br><span class="line">            is_driver_worker=is_driver_worker,</span><br><span class="line">            **speculative_args,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> model_runner_cls <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.model_runner = model_runner_cls(self.model_runner)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Uninitialized cache engine. Will be initialized by</span></span><br><span class="line">        <span class="comment"># initialize_cache.</span></span><br><span class="line">        self.cache_engine: <span class="type">List</span>[CacheEngine]</span><br><span class="line">        <span class="comment"># Initialize gpu_cache as pooling models don't initialize kv_caches</span></span><br><span class="line">        self.gpu_cache: <span class="type">Optional</span>[<span class="type">List</span>[<span class="type">List</span>[torch.Tensor]]] = <span class="literal">None</span></span><br><span class="line">        self._seq_group_metadata_cache: <span class="type">Dict</span>[<span class="built_in">str</span>, SequenceGroupMetadata] = {}</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_device</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化分布式环境。</span></span><br><span class="line"><span class="string">        1. torch.distributed.init_process_group, 设置 group;</span></span><br><span class="line"><span class="string">        2. model parallel initialized, 设置 TP, PP 的分布式 group 等；</span></span><br><span class="line"><span class="string">        3. ensure_kv_transfer_initialized.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_model</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.vllm_config.model_config.enable_sleep_mode:</span><br><span class="line">            allocator = CuMemAllocator.get_instance()</span><br><span class="line">            <span class="keyword">assert</span> allocator.get_current_usage() == <span class="number">0</span>, (</span><br><span class="line">                <span class="string">"Sleep mode can only be "</span></span><br><span class="line">                <span class="string">"used for one instance per process."</span>)</span><br><span class="line">            context = allocator.use_memory_pool(tag=<span class="string">"weights"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">from</span> contextlib <span class="keyword">import</span> nullcontext</span><br><span class="line">            context = nullcontext()</span><br><span class="line">        <span class="keyword">with</span> context:</span><br><span class="line">            <span class="comment"># worker 中调用 `load_model` 实际调用 `self.model_runner.load_model()`</span></span><br><span class="line">            <span class="comment"># 会实际将模型 load 进 device</span></span><br><span class="line">            self.model_runner.load_model()</span><br></pre></td></tr></table></figure>

<p><code>LLMEngine</code> 初始化中，<code>_initialize_kv_caches</code> 初始化 KV Cache，具体如下：</p>
<p>1.1 先计算可用的 GPU、CPU blocks 数量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vllm/worker/worker.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Worker</span>(<span class="title class_ inherited__">LocalOrDistributedWorkerBase</span>):</span><br><span class="line">    ...</span><br><span class="line"><span class="meta">    @torch.inference_mode()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">determine_num_available_blocks</span>(<span class="params">self</span>) -&gt; <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]:</span><br><span class="line">        free_memory_pre_profile, total_gpu_memory = torch.cuda.mem_get_info()</span><br><span class="line">        <span class="keyword">with</span> memory_profiling(</span><br><span class="line">                self.baseline_snapshot,</span><br><span class="line">                weights_memory=self.model_runner.model_memory_usage) <span class="keyword">as</span> result:</span><br><span class="line">            self.model_runner.profile_run()</span><br><span class="line">        <span class="comment"># 当前 vllm instance 可用显存 = 总的显存 * 指定系数</span></span><br><span class="line">        memory_for_current_instance = total_gpu_memory * \</span><br><span class="line">            self.cache_config.gpu_memory_utilization</span><br><span class="line">        <span class="comment"># kv cache 可用显存 = 当前 vllm instance 可用显存 - 当前 vllm instance 已使用的非 KV Cache 的显存</span></span><br><span class="line">        <span class="comment"># 1. model weights；</span></span><br><span class="line">        <span class="comment"># 2. 预留给 peak activation tensors 的；</span></span><br><span class="line">        <span class="comment"># 3. NCCL + buffers for some attention backends 等非 torch 组件占用的。</span></span><br><span class="line">        available_kv_cache_memory = (memory_for_current_instance -</span><br><span class="line">                                     result.non_kv_cache_memory)</span><br><span class="line">        <span class="comment"># Calculate the number of blocks that can be allocated with the</span></span><br><span class="line">        <span class="comment"># profiled peak memory.</span></span><br><span class="line">        cache_block_size = self.get_cache_block_size_bytes()</span><br><span class="line">        <span class="keyword">if</span> cache_block_size == <span class="number">0</span>:</span><br><span class="line">            num_gpu_blocks = <span class="number">0</span></span><br><span class="line">            num_cpu_blocks = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            num_gpu_blocks = <span class="built_in">int</span>(available_kv_cache_memory // cache_block_size)</span><br><span class="line">            num_cpu_blocks = <span class="built_in">int</span>(self.cache_config.swap_space_bytes //</span><br><span class="line">                                 cache_block_size)</span><br><span class="line">        num_gpu_blocks = <span class="built_in">max</span>(num_gpu_blocks, <span class="number">0</span>)</span><br><span class="line">        num_cpu_blocks = <span class="built_in">max</span>(num_cpu_blocks, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>计算 cache_block_size：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vllm/worker/cache_engine.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CacheEngine</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_cache_block_size</span>(<span class="params"></span></span><br><span class="line"><span class="params">        cache_config: CacheConfig,</span></span><br><span class="line"><span class="params">        model_config: ModelConfig,</span></span><br><span class="line"><span class="params">        parallel_config: ParallelConfig,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        计算 1 个 token 的 KV Cache 占用显存量。</span></span><br><span class="line"><span class="string">        对一个 token,</span></span><br><span class="line"><span class="string">            key: (B, num_heads, 1, head_size)</span></span><br><span class="line"><span class="string">            value: (B, num_heads, 1, head_size)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 以 Qwen2.5-7B-Instruct 为例，head_size=hidden_size/num_attention_heads=3584/28=128</span></span><br><span class="line">        head_size = model_config.get_head_size()</span><br><span class="line">        <span class="comment"># 以 Qwen2.5-7B-Instruct 为例，max(1, total_num_kv_heads // parallel_config.tensor_parallel_size)=4//1</span></span><br><span class="line">        num_heads = model_config.get_num_kv_heads(parallel_config)</span><br><span class="line">        <span class="comment"># 以 Qwen2.5-7B-Instruct 为例，num_attention_layers=28</span></span><br><span class="line">        num_attention_layers = model_config.get_num_layers_by_block_type(</span><br><span class="line">            parallel_config, LayerBlockType.attention)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cache_config.cache_dtype == <span class="string">"auto"</span>:</span><br><span class="line">            dtype = model_config.dtype</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dtype = STR_DTYPE_TO_TORCH_DTYPE[cache_config.cache_dtype]</span><br><span class="line"></span><br><span class="line">        key_cache_entry = num_heads * head_size</span><br><span class="line">        <span class="keyword">if</span> CacheEngine._align_cache(model_config):</span><br><span class="line">            key_cache_entry = align_to_256bytes(key_cache_entry,</span><br><span class="line">                                                model_config.dtype)</span><br><span class="line">        <span class="comment"># For MLA there is no value cache, since the latent vector</span></span><br><span class="line">        <span class="comment"># is joint keys and values.</span></span><br><span class="line">        value_cache_entry = key_cache_entry <span class="keyword">if</span> <span class="keyword">not</span> model_config.use_mla <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        <span class="comment"># cache_config.block_size 即是 slot 的数量。</span></span><br><span class="line">        total = num_attention_layers * cache_config.block_size * \</span><br><span class="line">            (key_cache_entry + value_cache_entry)</span><br><span class="line"></span><br><span class="line">        dtype_size = get_dtype_size(dtype)</span><br><span class="line">        <span class="keyword">return</span> dtype_size * total</span><br></pre></td></tr></table></figure>

<p>1.2 初始化 kv cache</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vllm/worker/worker.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Worker</span>(<span class="title class_ inherited__">LocalOrDistributedWorkerBase</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_cache_engine</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">assert</span> self.cache_config.num_gpu_blocks <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 初始化 cache_engine</span></span><br><span class="line">        self.cache_engine = [</span><br><span class="line">            CacheEngine(self.cache_config, self.model_config,</span><br><span class="line">                        self.parallel_config, self.device_config)</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.parallel_config.pipeline_parallel_size)</span><br><span class="line">        ]</span><br><span class="line">        <span class="comment"># 调用 _allocate_kv_cache 得到</span></span><br><span class="line">        self.gpu_cache: <span class="type">List</span>[<span class="type">List</span>[torch.Tensor]] = [</span><br><span class="line">            self.cache_engine[ve].gpu_cache</span><br><span class="line">            <span class="keyword">for</span> ve <span class="keyword">in</span> <span class="built_in">range</span>(self.parallel_config.pipeline_parallel_size)</span><br><span class="line">        ]</span><br><span class="line">        bind_kv_cache(self.compilation_config.static_forward_context,</span><br><span class="line">                      self.gpu_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_allocate_kv_cache</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        num_blocks: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        device: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">List</span>[torch.Tensor]:</span><br><span class="line">        <span class="string">"""Allocates KV cache on the specified device."""</span></span><br><span class="line">        <span class="comment"># vllm/attention/backends/flash_attn.py</span></span><br><span class="line">        <span class="comment"># FlashAttentionBackend 得到：</span></span><br><span class="line">        <span class="comment">#     (2, num_blocks, block_size, num_kv_heads, head_size)</span></span><br><span class="line">        kv_cache_shape = self.attn_backend.get_kv_cache_shape(</span><br><span class="line">            num_blocks, self.block_size, self.num_kv_heads, self.head_size)</span><br><span class="line"></span><br><span class="line">        kv_cache: <span class="type">List</span>[torch.Tensor] = []</span><br><span class="line">        alloc_shape = kv_cache_shape</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_attention_layers):</span><br><span class="line">            <span class="comment"># null block in CpuGpuBlockAllocator requires at least that</span></span><br><span class="line">            <span class="comment"># block to be zeroed-out.</span></span><br><span class="line">            <span class="comment"># We zero-out everything for simplicity.</span></span><br><span class="line">            layer_kv_cache = torch.zeros(alloc_shape,</span><br><span class="line">                                         dtype=self.dtype,</span><br><span class="line">                                         pin_memory=pin_memory,</span><br><span class="line">                                         device=device)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># view back to (TOTAL_PAGES, PAGE_SIZE, entry_shape...) for cases</span></span><br><span class="line">            <span class="comment"># when entry_shape is higher than 1D</span></span><br><span class="line">            kv_cache.append(layer_kv_cache.view(kv_cache_shape))</span><br><span class="line">        <span class="comment"># size: [(2, num_blocks, block_size, num_kv_heads, head_size)] * num_layers</span></span><br><span class="line">        <span class="keyword">return</span> kv_cache</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>Scheduler 初始化</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vllm/core/scheduler.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Scheduler</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        scheduler_config: SchedulerConfig,</span></span><br><span class="line"><span class="params">        cache_config: CacheConfig,</span></span><br><span class="line"><span class="params">        lora_config: <span class="type">Optional</span>[LoRAConfig],</span></span><br><span class="line"><span class="params">        pipeline_parallel_size: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">        output_proc_callback: <span class="type">Optional</span>[<span class="type">Callable</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># BlockSpaceManagerImpl 默认为：vllm.core.block_manager.SelfAttnBlockSpaceManager</span></span><br><span class="line">        <span class="comment"># in vllm/core/block_manager.py</span></span><br><span class="line">        self.block_manager = BlockSpaceManagerImpl(</span><br><span class="line">            block_size=self.cache_config.block_size,</span><br><span class="line">            num_gpu_blocks=num_gpu_blocks,</span><br><span class="line">            num_cpu_blocks=num_cpu_blocks,</span><br><span class="line">            sliding_window=self.cache_config.sliding_window,</span><br><span class="line">            enable_caching=self.cache_config.enable_prefix_caching,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># Sequence groups in the WAITING state.</span></span><br><span class="line">        <span class="comment"># Contain new prefill or preempted requests.</span></span><br><span class="line">        self.waiting: Deque[SequenceGroup] = deque()</span><br><span class="line">        <span class="comment"># Sequence groups in the RUNNING state.</span></span><br><span class="line">        <span class="comment"># Contain decode requests.</span></span><br><span class="line">        self.running: Deque[SequenceGroup] = deque()</span><br><span class="line">        <span class="comment"># Sequence groups in the SWAPPED state.</span></span><br><span class="line">        <span class="comment"># Contain decode requests that are swapped out.</span></span><br><span class="line">        self.swapped: Deque[SequenceGroup] = deque()</span><br><span class="line">        <span class="comment"># Sequence groups finished requests ids since last step iteration.</span></span><br><span class="line">        <span class="comment"># It lets the model know that any state associated with these requests</span></span><br><span class="line">        <span class="comment"># can and must be released after the current step.</span></span><br><span class="line">        <span class="comment"># This is used to evict the finished requests from the Mamba cache.</span></span><br><span class="line">        self._finished_requests_ids: <span class="type">List</span>[<span class="built_in">str</span>] = <span class="built_in">list</span>()</span><br></pre></td></tr></table></figure>


<h2 id="LLM-generate-逻辑"><a href="#LLM-generate-逻辑" class="headerlink" title="LLM generate 逻辑"></a>LLM generate 逻辑</h2><p>generate 的逻辑流程：</p>
<ol>
<li>for-loop 每个 prompt、sampling_param，并将 prompt、sampling_param 作为 request 参数添加给 <code>llm_engine</code> 的 request pool。<br> a. input_processor 做一些输入 prompt 的处理；<br> b. 将处理过的 prompt (tokens) 封装成 <code>Sequence</code>，seq 再封装进 <code>SequenceGroup</code>；<br> c. 将封装成的 <code>seq_group</code> append 到 <code>scheduler</code> 的 <code>waiting</code> 队列，以待后续处理任务的使用。</li>
</ol>
<p>上述流程涉及的关键代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># entrypoints/llm.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LLM</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_add_request</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        prompt: PromptType,</span></span><br><span class="line"><span class="params">        params: <span class="type">Union</span>[SamplingParams, PoolingParams],</span></span><br><span class="line"><span class="params">        lora_request: <span class="type">Optional</span>[LoRARequest] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        prompt_adapter_request: <span class="type">Optional</span>[PromptAdapterRequest] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        priority: <span class="built_in">int</span> = <span class="number">0</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        request_id = <span class="built_in">str</span>(<span class="built_in">next</span>(self.request_counter))</span><br><span class="line">        self.llm_engine.add_request(</span><br><span class="line">            request_id,</span><br><span class="line">            prompt,</span><br><span class="line">            params,</span><br><span class="line">            lora_request=lora_request,</span><br><span class="line">            prompt_adapter_request=prompt_adapter_request,</span><br><span class="line">            priority=priority,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="comment"># engine/llm_engine.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LLMEngine</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_add_processed_request</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        request_id: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">        processed_inputs: ProcessorInputs,</span></span><br><span class="line"><span class="params">        params: <span class="type">Union</span>[SamplingParams, PoolingParams],</span></span><br><span class="line"><span class="params">        arrival_time: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">        lora_request: <span class="type">Optional</span>[LoRARequest],</span></span><br><span class="line"><span class="params">        prompt_adapter_request: <span class="type">Optional</span>[PromptAdapterRequest],</span></span><br><span class="line"><span class="params">        trace_headers: <span class="type">Optional</span>[Mapping[<span class="built_in">str</span>, <span class="built_in">str</span>]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        priority: <span class="built_in">int</span> = <span class="number">0</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Optional</span>[SequenceGroup]:</span><br><span class="line">        ...</span><br><span class="line">        block_size = self.cache_config.block_size</span><br><span class="line">        seq_id = <span class="built_in">next</span>(self.seq_counter)</span><br><span class="line">        eos_token_id = self.input_preprocessor.get_eos_token_id(lora_request)</span><br><span class="line">        ...</span><br><span class="line">        seq = <span class="type">Sequence</span>(seq_id, decoder_inputs, block_size, eos_token_id,</span><br><span class="line">                       lora_request, prompt_adapter_request)</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(params, SamplingParams):</span><br><span class="line">            <span class="comment"># 由 seq 创建 seq_group；</span></span><br><span class="line">            <span class="comment"># seq_group 由有相同 prompt 的 seq 组合而成，如：采样策略为 beam search。</span></span><br><span class="line">            seq_group = self._create_sequence_group_with_sampling(</span><br><span class="line">                request_id,</span><br><span class="line">                seq,</span><br><span class="line">                params,</span><br><span class="line">                arrival_time=arrival_time,</span><br><span class="line">                lora_request=lora_request,</span><br><span class="line">                trace_headers=trace_headers,</span><br><span class="line">                prompt_adapter_request=prompt_adapter_request,</span><br><span class="line">                encoder_seq=encoder_seq,</span><br><span class="line">                priority=priority)</span><br><span class="line">        ...</span><br><span class="line">        costs = [</span><br><span class="line">            scheduler.get_num_unfinished_seq_groups()</span><br><span class="line">            <span class="keyword">for</span> scheduler <span class="keyword">in</span> self.scheduler</span><br><span class="line">        ]</span><br><span class="line">        min_cost_scheduler = self.scheduler[costs.index(<span class="built_in">min</span>(costs))]</span><br><span class="line">        <span class="comment"># 将 seq_group append 到 waiting 队列。</span></span><br><span class="line">        min_cost_scheduler.add_seq_group(seq_group)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> seq_group</span><br></pre></td></tr></table></figure>

<p><code>Sequence</code> 和 <code>SequenceGroup</code> 类关系图如下：</p>
<img src="vllm_seq_class.png" width="80%">

<ol start="2">
<li>然后调用 <code>self._run_engine()</code> 进行实际的调度和推理。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># entrypoints/llm.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LLM</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_run_engine</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, *, use_tqdm: <span class="built_in">bool</span></span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">List</span>[<span class="type">Union</span>[RequestOutput, PoolingRequestOutput]]:</span><br><span class="line">        outputs: <span class="type">List</span>[<span class="type">Union</span>[RequestOutput, PoolingRequestOutput]] = []</span><br><span class="line">        total_in_toks = <span class="number">0</span></span><br><span class="line">        total_out_toks = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 调用 scheduler，检查 `waiting`，`running`，`swapped` 队列是否有非空的，</span></span><br><span class="line">        <span class="comment"># 如果有，则表示有 unfinished_requests，继续 while 循环。</span></span><br><span class="line">        <span class="keyword">while</span> self.llm_engine.has_unfinished_requests():</span><br><span class="line">            step_outputs = self.llm_engine.step()</span><br><span class="line">            <span class="keyword">for</span> output <span class="keyword">in</span> step_outputs:</span><br><span class="line">                <span class="keyword">if</span> output.finished:</span><br><span class="line">                    outputs.append(output)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Sort the outputs by request ID.</span></span><br><span class="line">        <span class="comment"># This is necessary because some requests may be finished earlier than</span></span><br><span class="line">        <span class="comment"># its previous requests.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sorted</span>(outputs, key=<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x.request_id))</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><code>llm_engine</code> 调用 <code>step</code>，<code>scheduler</code> 负责调度，<code>model_executor</code> 负责实际的推理。</li>
</ol>
<h2 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h2><p>总的调度逻辑如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># core/scheduler.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Scheduler</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">schedule</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Tuple</span>[<span class="type">List</span>[SequenceGroupMetadata], SchedulerOutputs, <span class="built_in">bool</span>]:</span><br><span class="line">        <span class="comment"># Schedule sequence groups.</span></span><br><span class="line">        <span class="comment"># This function call changes the internal states of the scheduler</span></span><br><span class="line">        <span class="comment"># such as self.running, self.swapped, and self.waiting.</span></span><br><span class="line">        scheduler_outputs: SchedulerOutputs = self._schedule()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 不开启 chunked_prefill 的情况</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_schedule_default</span>(<span class="params">self</span>) -&gt; SchedulerOutputs:</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># If any requests are swapped, prioritized swapped requests.</span></span><br><span class="line">        <span class="comment"># 优先调度 swapped 队列，即使 waiting 队列非空</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.swapped:</span><br><span class="line">            prefills = self._schedule_prefills(budget,</span><br><span class="line">                                               curr_loras,</span><br><span class="line">                                               enable_chunking=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 如果 prefill 中没有 seq_groups 且调度策略为 'priority' 时调度优先级抢占的 seq</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(prefills.seq_groups</span><br><span class="line">               ) == <span class="number">0</span> <span class="keyword">and</span> self.scheduler_config.policy == <span class="string">"priority"</span>:</span><br><span class="line">            self._schedule_priority_preemption(budget)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Don't schedule decodes if prefills are scheduled.</span></span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> If `_schedule_prefills` doesn't enable chunking, self.running</span></span><br><span class="line">        <span class="comment"># only contains decode requests, not chunked prefills.</span></span><br><span class="line">        <span class="comment"># prefill 的 seq_groups 为空才会调度 running 或 swapped 队列中的 seq。</span></span><br><span class="line">        <span class="comment"># 即：prefill 和 decode 是分开调度的。</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(prefills.seq_groups) == <span class="number">0</span>:</span><br><span class="line">            running_scheduled = self._schedule_running(budget,</span><br><span class="line">                                                       curr_loras,</span><br><span class="line">                                                       enable_chunking=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># If any sequence group is preempted, do not swap in any sequence</span></span><br><span class="line">            <span class="comment"># group. because it means there's no slot for new running requests.</span></span><br><span class="line">            <span class="comment"># 如果 preempted 和 swapped_out 为空，说明 running_scheduled 中的 seq_group 都能正常分配 kv cahche 的空间，</span></span><br><span class="line">            <span class="comment"># 则可以将 swapped 队列中的 seq_group swap_in 进行调度。</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">len</span>(running_scheduled.preempted) +</span><br><span class="line">                    <span class="built_in">len</span>(running_scheduled.swapped_out) == <span class="number">0</span>):</span><br><span class="line">                swapped_in = \</span><br><span class="line">                    self._schedule_swapped(budget, curr_loras)</span><br></pre></td></tr></table></figure>

<p>关于 prefill 阶段的调度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># core/scheduler.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Scheduler</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_schedule_prefills</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        budget: SchedulingBudget,</span></span><br><span class="line"><span class="params">        curr_loras: <span class="type">Optional</span>[<span class="type">Set</span>[<span class="built_in">int</span>]],</span></span><br><span class="line"><span class="params">        enable_chunking: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        partial_prefill_metadata: <span class="type">Optional</span>[PartialPrefillMetadata] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; SchedulerPrefillOutputs:</span><br><span class="line">    ...</span><br><span class="line">        <span class="keyword">while</span> self._passed_delay(time.time()) <span class="keyword">and</span> waiting_queue:</span><br><span class="line">            <span class="comment"># fcfs 调度方式，先进先调度</span></span><br><span class="line">            seq_group = waiting_queue[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            waiting_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)</span><br><span class="line">            ...</span><br><span class="line">            <span class="comment"># prefill 阶段，num_new_tokens_cached 为 0；num_new_tokens_uncached 为 prompt 的长度。</span></span><br><span class="line">            num_new_tokens_uncached, num_new_tokens_cached = (</span><br><span class="line">                self._get_num_new_uncached_and_cached_tokens(</span><br><span class="line">                    seq_group,</span><br><span class="line">                    SequenceStatus.WAITING,</span><br><span class="line">                    enable_chunking,</span><br><span class="line">                    budget,</span><br><span class="line">                    partial_prefill_metadata=partial_prefill_metadata,</span><br><span class="line">                ))</span><br><span class="line">            <span class="comment"># 如果 num_new_tokens &gt; scheduler 默认设置的最大值（如：32768）</span></span><br><span class="line">            <span class="comment"># 则该 seq_group 中的 seq 都将被设置为 SequenceStatus.FINISHED_IGNORED</span></span><br><span class="line">            num_new_tokens = num_new_tokens_uncached + num_new_tokens_cached</span><br><span class="line">            ...</span><br><span class="line">            <span class="comment"># If the sequence group cannot be allocated, stop.</span></span><br><span class="line">            <span class="comment"># 判断当前 seq_group 是否有可分配的 KV Cache（由 block manager 管理），下面会详细介绍。</span></span><br><span class="line">            can_allocate = self.block_manager.can_allocate(</span><br><span class="line">                seq_group, num_lookahead_slots=num_lookahead_slots)</span><br><span class="line">            <span class="keyword">if</span> can_allocate == AllocStatus.LATER:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">elif</span> can_allocate == AllocStatus.NEVER:</span><br><span class="line">                logger.warning(</span><br><span class="line">                    <span class="string">"Input prompt (%d tokens) + lookahead slots (%d) is "</span></span><br><span class="line">                    <span class="string">"too long and exceeds the capacity of block_manager"</span>,</span><br><span class="line">                    num_new_tokens,</span><br><span class="line">                    num_lookahead_slots,</span><br><span class="line">                )</span><br><span class="line">                <span class="keyword">for</span> seq <span class="keyword">in</span> waiting_seqs:</span><br><span class="line">                    seq.status = SequenceStatus.FINISHED_IGNORED</span><br><span class="line">                ignored_seq_groups.append(seq_group)</span><br><span class="line">                waiting_queue.popleft()</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            ...</span><br><span class="line">            <span class="comment"># 将 waiting 队列最先进入的 seq_group pop 出来，</span></span><br><span class="line">            <span class="comment"># 为该 seq_group 分配 kv cache blocks，</span></span><br><span class="line">            <span class="comment"># 并且将该 seq_group 中的 seq 状态由 SequenceStatus.WAITING 设置为 SequenceStatus.RUNNING。</span></span><br><span class="line">            waiting_queue.popleft()</span><br><span class="line">            self._allocate_and_set_running(seq_group)</span><br><span class="line">            ...</span><br><span class="line">            seq_groups.append(</span><br><span class="line">                ScheduledSequenceGroup(seq_group=seq_group,</span><br><span class="line">                                       token_chunk_size=num_new_tokens))</span><br><span class="line">            <span class="comment"># 更新 budget 状态</span></span><br><span class="line">            <span class="comment"># 一个是总的 batched tokens 一个是处理的 seq 数量</span></span><br><span class="line">            budget.add_num_batched_tokens(</span><br><span class="line">                seq_group.request_id,</span><br><span class="line">                num_batched_tokens=num_new_tokens_uncached,</span><br><span class="line">                num_cached_tokens=num_new_tokens_cached,</span><br><span class="line">            )</span><br><span class="line">            budget.add_num_seqs(seq_group.request_id, num_new_seqs)</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span> SchedulerPrefillOutputs(</span><br><span class="line">            seq_groups=seq_groups,</span><br><span class="line">            ignored_seq_groups=ignored_seq_groups,</span><br><span class="line">            num_lookahead_slots=self._get_num_lookahead_slots(</span><br><span class="line">                is_prefill=<span class="literal">True</span>, enable_chunking=enable_chunking),</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<p>关于 decode 阶段调度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># core/scheduler.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Scheduler</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_schedule_running</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        budget: SchedulingBudget,</span></span><br><span class="line"><span class="params">        curr_loras: <span class="type">Optional</span>[<span class="type">Set</span>[<span class="built_in">int</span>]],</span></span><br><span class="line"><span class="params">        enable_chunking: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        partial_prefill_metadata: <span class="type">Optional</span>[PartialPrefillMetadata] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; SchedulerRunningOutputs:</span><br><span class="line">    ...</span><br><span class="line">        <span class="keyword">while</span> running_queue:</span><br><span class="line">            seq_group = running_queue[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># We discard the cached tokens info here because we don't need it</span></span><br><span class="line">            <span class="comment"># for running sequence:</span></span><br><span class="line">            <span class="comment">#   1. If a sequence is running with chunked prefill, the cached</span></span><br><span class="line">            <span class="comment">#      tokens info was already used for the first prefill.</span></span><br><span class="line">            <span class="comment">#   2. If a sequence is running with non-chunked prefill, then</span></span><br><span class="line">            <span class="comment">#      there it's a decoding sequence, and the cached tokens info is</span></span><br><span class="line">            <span class="comment">#      irrelevant.</span></span><br><span class="line">            num_uncached_new_tokens, _ = \</span><br><span class="line">                self._get_num_new_uncached_and_cached_tokens(</span><br><span class="line">                seq_group,</span><br><span class="line">                SequenceStatus.RUNNING,</span><br><span class="line">                enable_chunking,</span><br><span class="line">                budget,</span><br><span class="line">                partial_prefill_metadata,</span><br><span class="line">            )</span><br><span class="line">            running_queue.popleft()</span><br><span class="line">            ...</span><br><span class="line">            <span class="keyword">while</span> <span class="keyword">not</span> self._can_append_slots(seq_group, enable_chunking):</span><br><span class="line">                budget.subtract_num_batched_tokens(seq_group.request_id,</span><br><span class="line">                                                   num_running_tokens)</span><br><span class="line">                num_running_seqs = seq_group.get_max_num_running_seqs()</span><br><span class="line">                budget.subtract_num_seqs(seq_group.request_id,</span><br><span class="line">                                         num_running_seqs)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (curr_loras <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> seq_group.lora_int_id &gt; <span class="number">0</span></span><br><span class="line">                        <span class="keyword">and</span> seq_group.lora_int_id <span class="keyword">in</span> curr_loras):</span><br><span class="line">                    curr_loras.remove(seq_group.lora_int_id)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Determine victim sequence</span></span><br><span class="line">                cont_loop = <span class="literal">True</span></span><br><span class="line">                <span class="comment"># 如果 running 的队列非空，但是没有空间分配 kv cache，</span></span><br><span class="line">                <span class="comment"># 就将 running 队列的最后一个 seq_group pop 出来作为抢占或者 swapped out，然后循环直到满足退出条件</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 如果 running 的队列为空，没有可分配 kv cache 的空间，则将当前 seq_group 作为抢占或者 swapped out</span></span><br><span class="line">                <span class="keyword">if</span> running_queue:</span><br><span class="line">                    <span class="comment"># Preempt the lowest-priority sequence group.</span></span><br><span class="line">                    victim_seq_group = running_queue.pop()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># No other sequence group can be preempted.</span></span><br><span class="line">                    <span class="comment"># Preempt the current sequence group.</span></span><br><span class="line">                    <span class="comment"># Note: This is also where we stop this loop</span></span><br><span class="line">                    <span class="comment"># (since there is nothing else to preempt)</span></span><br><span class="line">                    victim_seq_group = seq_group</span><br><span class="line">                    cont_loop = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># With async postprocessor, before preempting a sequence</span></span><br><span class="line">                <span class="comment"># we need to ensure it has no pending async postprocessor</span></span><br><span class="line">                do_preempt = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">if</span> self.use_async_output_proc:</span><br><span class="line">                    <span class="keyword">assert</span> self.output_proc_callback <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">                    self.output_proc_callback(</span><br><span class="line">                        request_id=victim_seq_group.request_id)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># It may be that the async pending "victim_seq_group"</span></span><br><span class="line">                    <span class="comment"># becomes finished, in which case we simply free it.</span></span><br><span class="line">                    <span class="keyword">if</span> victim_seq_group.is_finished():</span><br><span class="line">                        self._free_finished_seq_group(victim_seq_group)</span><br><span class="line">                        do_preempt = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># Do preemption</span></span><br><span class="line">                <span class="keyword">if</span> do_preempt:</span><br><span class="line">                    <span class="comment"># 决定抢占模式 SWAP or RECOMPUTE</span></span><br><span class="line">                    preempted_mode = self._preempt(victim_seq_group,</span><br><span class="line">                                                   blocks_to_swap_out)</span><br><span class="line">                    <span class="keyword">if</span> preempted_mode == PreemptionMode.RECOMPUTE:</span><br><span class="line">                        preempted.append(victim_seq_group)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        swapped_out.append(victim_seq_group)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> cont_loop:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 分配 slot，后续进行 decode</span></span><br><span class="line">                self._append_slots(seq_group, blocks_to_copy, enable_chunking)</span><br></pre></td></tr></table></figure>

<p>swapped 的调度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># core/scheduler.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Scheduler</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_schedule_swapped</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        budget: SchedulingBudget,</span></span><br><span class="line"><span class="params">        curr_loras: <span class="type">Optional</span>[<span class="type">Set</span>[<span class="built_in">int</span>]],</span></span><br><span class="line"><span class="params">        enable_chunking: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; SchedulerSwappedInOutputs:</span><br><span class="line">    ...</span><br><span class="line">        blocks_to_swap_in: <span class="type">List</span>[<span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]] = []</span><br><span class="line">        blocks_to_copy: <span class="type">List</span>[<span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]] = []</span><br><span class="line">        decode_seq_groups: <span class="type">List</span>[ScheduledSequenceGroup] = []</span><br><span class="line">        prefill_seq_groups: <span class="type">List</span>[ScheduledSequenceGroup] = []</span><br><span class="line">        infeasible_seq_groups: <span class="type">List</span>[SequenceGroup] = []</span><br><span class="line"></span><br><span class="line">        swapped_queue = self.swapped</span><br><span class="line"></span><br><span class="line">        leftover_swapped: Deque[SequenceGroup] = deque()</span><br><span class="line">        <span class="keyword">while</span> swapped_queue:</span><br><span class="line">            seq_group = swapped_queue[<span class="number">0</span>]</span><br><span class="line">            swapped_queue.popleft()</span><br><span class="line"></span><br><span class="line">            self._swap_in(seq_group, blocks_to_swap_in)</span><br><span class="line">            self._append_slots(seq_group, blocks_to_copy, enable_chunking)</span><br><span class="line">            ...</span><br></pre></td></tr></table></figure>

<h3 id="block-manager"><a href="#block-manager" class="headerlink" title="block manager"></a>block manager</h3><p>在上述调度中，有一个很重要的判断是 kv cache 的分配情况，这个由 <code>block_manager</code> 实际管理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vllm/core/block_manager.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SelfAttnBlockSpaceManager</span>(<span class="title class_ inherited__">BlockSpaceManager</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        block_size: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        num_gpu_blocks: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        num_cpu_blocks: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        watermark: <span class="built_in">float</span> = <span class="number">0.01</span>,</span></span><br><span class="line"><span class="params">        sliding_window: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        enable_caching: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.block_size = block_size</span><br><span class="line">        self.num_total_gpu_blocks = num_gpu_blocks</span><br><span class="line">        self.num_total_cpu_blocks = num_cpu_blocks</span><br><span class="line">        <span class="comment"># memory swapping 的阈值，默认 0.01</span></span><br><span class="line">        self.watermark = watermark</span><br><span class="line">        self.watermark_blocks = <span class="built_in">int</span>(watermark * num_gpu_blocks)</span><br><span class="line">        <span class="comment"># CpuGpuBlockAllocator in vllm/core/block/cpu_gpu_block_allocator.py</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># BlockAllocator 创建 cpu_allocator 和 gpu_allocator</span></span><br><span class="line">        <span class="comment"># 默认 cpu_allocator 和 gpu_allocator 都是 `NaiveBlockAllocator` 的实例</span></span><br><span class="line">        <span class="comment"># CpuGpuBlockAllocator(</span></span><br><span class="line">        <span class="comment">#     cpu_block_allocator=cpu_allocator,</span></span><br><span class="line">        <span class="comment">#     gpu_block_allocator=gpu_allocator,</span></span><br><span class="line">        <span class="comment"># )</span></span><br><span class="line">        self.block_allocator = CpuGpuBlockAllocator.create(</span><br><span class="line">            allocator_type=<span class="string">"prefix_caching"</span> <span class="keyword">if</span> enable_caching <span class="keyword">else</span> <span class="string">"naive"</span>,</span><br><span class="line">            num_gpu_blocks=num_gpu_blocks,</span><br><span class="line">            num_cpu_blocks=num_cpu_blocks,</span><br><span class="line">            block_size=block_size,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># BlockTable in vllm/core/block/block_table.py</span></span><br><span class="line">        self.block_tables: <span class="type">Dict</span>[SeqId, BlockTable] = {}</span><br><span class="line">        self.cross_block_tables: <span class="type">Dict</span>[EncoderSeqId, BlockTable] = {}</span><br></pre></td></tr></table></figure>

<p>block manager 中涉及的几个重要类的关系如下：</p>
<img src="block_manager.png" width="80%">


<p>scheduler 中 <code>_schedule_prefills</code> 时，会调用 <code>self.block_manager.can_allocate</code> 判断是否能够成功分配 block 给 seq_group：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SelfAttnBlockSpaceManager</span>(<span class="title class_ inherited__">BlockSpaceManager</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">can_allocate</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                     seq_group: SequenceGroup,</span></span><br><span class="line"><span class="params">                     num_lookahead_slots: <span class="built_in">int</span> = <span class="number">0</span></span>) -&gt; AllocStatus:</span><br><span class="line">        seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># seq tokens num / block_size</span></span><br><span class="line">        num_required_blocks = BlockTable.get_num_required_blocks(</span><br><span class="line">            seq.get_token_ids(),</span><br><span class="line">            block_size=self.block_size,</span><br><span class="line">            num_lookahead_slots=num_lookahead_slots,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 实际调用 NaiveBlockAllocator 的 get_num_free_blocks</span></span><br><span class="line">        num_free_gpu_blocks = self.block_allocator.get_num_free_blocks(</span><br><span class="line">            device=Device.GPU)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Use watermark to avoid frequent cache eviction.</span></span><br><span class="line">        <span class="keyword">if</span> (self.num_total_gpu_blocks - num_required_blocks</span><br><span class="line">                &lt; self.watermark_blocks):</span><br><span class="line">            <span class="keyword">return</span> AllocStatus.NEVER</span><br><span class="line">        <span class="keyword">if</span> num_free_gpu_blocks - num_required_blocks &gt;= self.watermark_blocks:</span><br><span class="line">            <span class="keyword">return</span> AllocStatus.OK</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> AllocStatus.LATER</span><br></pre></td></tr></table></figure>


<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li>vllm paper. <a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.06180">arxiv<i class="fas fa-external-link-alt"></i></a></li>
<li>vLLM First SF Meetup Slides (Public). <a class="link" target="_blank" rel="noopener" href="https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?pli=1#slide=id.p">slide<i class="fas fa-external-link-alt"></i></a></li>
<li>知乎专栏：<a class="link" target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/680153425">https://zhuanlan.zhihu.com/p/680153425<i class="fas fa-external-link-alt"></i></a></li>
</ul>

                </div>

                

                <div class="post-bottom-tags-and-share border-box">
                    <div>
                        
                            <ul class="post-tags-box border-box">
                                
                                    <li class="tag-item border-box">
                                        <i class="icon fas fa-hashtag"></i>&nbsp;<a href="/tags/vLLM/">vLLM</a>
                                    </li>
                                
                            </ul>
                        
                    </div>
                    <div>
                        
                    </div>
                </div>

                

                
                    <div class="article-nav">
                        
                        
                            <div class="article-next">
                                <a class="next"
                                   rel="next"
                                   href="/2025/04/13/algorithm-basics-of-rl-1/"
                                   title="Basics of Reinforcement Learning (part 2)"
                                >
                                    <span class="title flex-center">
                                        <span class="post-nav-title-item text-ellipsis">Basics of Reinforcement Learning (part 2)</span>
                                        <span class="post-nav-item">Next posts</span>
                                    </span>
                                            <span class="right arrow-icon flex-center">
                                      <i class="fas fa-chevron-right"></i>
                                    </span>
                                </a>
                            </div>
                        
                    </div>
                

                
                    






                
            </div>
        </div>

        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom border-box">
            
<footer class="footer border-box">
    <div class="border-box website-info-box default">
        
            <div class="copyright-info info-item default">
                &copy;&nbsp;<span>2016</span>&nbsp;-&nbsp;2025
                
                    &nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;&nbsp;<a href="/">muzhi :) 木之</a>
                
            </div>

            <!-- <div class="theme-info info-item default">
                Powered by&nbsp;<a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;&&nbsp;Theme&nbsp;<a class="keep-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep</a>
            </div> -->

            

            
        

        <div class="count-item info-item default">
            

            

            
        </div>
    </div>
</footer>

        </div>
    </div>

    <!-- post tools -->
    
        <div class="post-tools right-toc">
            <div class="post-tools-container border-box">
    <ul class="tools-list border-box">
        <!-- PC TOC show toggle -->
        

        <!-- PC go comment -->
        
    </ul>
</div>

        </div>
    

    <!-- side tools -->
    <div class="side-tools">
        <div class="side-tools-container border-box ">
    <ul class="side-tools-list side-tools-show-handle border-box">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list border-box">
        

        

        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>

        <li class="tools-item tool-scroll-to-top flex-center show-arrow">
            <i class="arrow fas fa-arrow-up"></i>
            <span class="percent"></span>
        </li>
    </ul>
</div>

    </div>

    <!-- image mask -->
    <div class="zoom-in-image-mask">
    <img class="zoom-in-image">
</div>


    <!-- local search -->
    

    <!-- tablet toc -->
    
</main>



<!-- common -->

<script src="/js/utils.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>

<script src="/js/main.js"></script>

<script src="/js/libs/anime.min.js"></script>


<!-- local-search -->


<!-- code-block -->


<!-- lazyload -->


<div class="">
    
        <!-- post-helper -->
        
<script src="/js/post/post-helper.js"></script>


        <!-- toc -->
        

        <!-- copyright-info -->
        

        <!-- share -->
        
    

    <!-- category-page -->
    

    <!-- links-page -->
    
</div>

<!-- pjax -->



</body>
</html>
